{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Spark + Python\n",
    "\n",
    "This guide assumes you have a Ubuntu Linux.  This is verified on Ubuntu 20.04.  But will probably work on recent Ubuntu Linux.\n",
    "\n",
    "If you have other system like MacOS, please adjust accordingly.\n",
    "\n",
    "## Step-1: Install JDK-11\n",
    "\n",
    "We need JDK (Java Development Kit) not JRE (Java runtime environment), in-order  to run Spark.\n",
    "\n",
    "```bash\n",
    "$   sudo apt update\n",
    "$   sudo apt install -y openjdk-11-jdk\n",
    "```\n",
    "\n",
    "Verify this by \n",
    "\n",
    "```bash\n",
    "$   java -version\n",
    "```\n",
    "\n",
    "## Step-2: Install Anaconda Python\n",
    "\n",
    "Needed to run Spark Python applications.\n",
    "\n",
    "```bash\n",
    "$   wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh\n",
    "\n",
    "$    bash ./Anaconda3-2021.11-Linux-x86_64.sh\n",
    "# go through the steps\n",
    "```\n",
    "\n",
    "After install is complete, open a new terminal so changes can take effect.  And then verify as\n",
    "\n",
    "```bash\n",
    "$   conda --version\n",
    "\n",
    "$   python --version\n",
    "```\n",
    "\n",
    "Make sure the python version is from Anaconda.\n",
    "\n",
    "\n",
    "## Step-3: Setup Spark\n",
    "\n",
    "```bash\n",
    "$   cd  # be in home dir\n",
    "\n",
    "$   wget https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
    "\n",
    "$   tar xvf spark-3.2.0-bin-hadoop3.2.tgz\n",
    "\n",
    "$   mv spark-3.2.0-bin-hadoop3.2 spark\n",
    "\n",
    "```\n",
    "\n",
    "After this we will have spark installed in `~/spark`\n",
    "\n",
    "## Step-4: Test PySpark\n",
    "\n",
    "```bash\n",
    "$   ~/spark/bin/pyspark\n",
    "```\n",
    "\n",
    "This will drop you into PySpark shell.  Try the following\n",
    "\n",
    "```python\n",
    "spark.range(1,10).show()\n",
    "```\n",
    "\n",
    "If you see an output like this, you are good\n",
    "\n",
    "```text\n",
    "+---+\n",
    "| id|\n",
    "+---+\n",
    "|  1|\n",
    "|  2|\n",
    "|  3|\n",
    "|  4|\n",
    "|  5|\n",
    "|  6|\n",
    "|  7|\n",
    "|  8|\n",
    "|  9|\n",
    "+---+\n",
    "```\n",
    "\n",
    "## (Optional) Step-5: Jupyter and Spark\n",
    "\n",
    "Install following packages  (these are great for data analytics with Pyspark)\n",
    "\n",
    "```bash\n",
    "$   conda install numpy  pandas  matplotlib  seaborn  jupyter  jupyterlab\n",
    "$   conda install -c conda-forge findspark\n",
    "```\n",
    "\n",
    "Running Jupyter with Spark.\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=$HOME/spark\n",
    "jupyter lab\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
